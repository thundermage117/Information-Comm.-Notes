\documentclass{article}
\usepackage{amsmath}

\begin{document}
\begin{center}
\textbf{\huge{Week 3}}
\end{center}
\subsection{Joint Entropy in multiple RV's}

$$H(X_1, \cdots, X_n)= \sum_{ (x_1, \cdots, x_n) \in supp(P_{X_1, \cdots, X_n})} P(x_1, \cdots, x_n) \log_2 \frac{1}{P(x_1, \cdots, x_n)}$$

Where $P_{X,Y} := \mathcal{X} \times \mathcal{Y} \rightarrow \text{Cartesian Product}$.

$\mathcal{X} \times \mathcal{Y} := \{ (x,y): x \in \mathcal{X}, y\in \mathcal{Y}\} $

\subsection{Conditional Joint Entropy}
$$H(X,Y):= \sum_{(u,v) \in supp(P_{U,V})} P_{U,V}(u,v)H(X,Y/U=u,V=v) $$
where,
$$ H(X,Y/U=u,V=v) = \sum_{(x,y) \in supp(P_{X,Y/U=u,V=v})} P(x,y/u,v) \log_2 \frac{1}{P(x,y/u,v)}$$

This can be extended to any number of variables before and after the conditioning.

eg: $P(x_1,x_2,x_3/y_1,y_2)= P(x_1,x_2/x_3,y_1,y_2)+ P(x_3/x_1,x_2,y_1,y_2)$.
\section{Chain Rule for Joint Entropy}

Lemma:
\begin{multline}
    H(X_1, \cdots, X_n)= H(X_1)+ H(X_2 / X_1)+ H(X_3/ X_1, X_2)+ \cdots \\ + H(X_n/ X_1, \cdots, X_n)
\end{multline}
Proof:
\begin{align*}
    P(x_1,\cdots,x_n)&=P(x_1)P(x_2,\cdots,x_n/x_1) \\
    &= P(x_1)P(x_2/x_1)P(x_3,\cdots,x_n/x_1,x_2) \\
    &= P(x_1)P(x_2/x_1)P(x_3/x_1,x_2)P(x_4,\cdots,x_n/x_1,x_2,x_3) \\
    &= P(x_1)P(x_2/x_1)P(x_3/x_1,x_2)\cdots P(x_n/x_1,\cdots,x_{n-1})
\end{align*}
%use definition of joint entropy to complete the proof.





\end{document}
